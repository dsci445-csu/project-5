---
title: "Illinois DOC Sentencing Machine Learning Analysis"
author: Juan Gonzalez, Aaron Graff, Ilijah Pearson, Hope Winsor
format: pptx
editor: visual
---

## Dataset and Preparation

Our data comes from Kaggle, with data scraped off the Illinois Department Of Corrections website.

::: incremental
-   Contains information 61,052 individuals who have been incarcerated in Illinois from 1952-2018.

-   "Person" dataset includes simple descriptive information on the person: Name, DOB, Sentencing Date, Race, Eye Color, Hair Color, etc...

-   "Marks" has multiple rows per person listing any additional descriptive features like scars and tattoos.

-   "Sentencing" is information on the charges faced by the individual, the crime categorized by Illinois' felony classes, and our response variable, sentencing time.
:::

## Research Questions

::: incremental
-   What predictors have the strongest effect on sentencing length?

-   How accurately can we predict sentencing length?

-   What predictors have the strongest effect on offence category?

-   How accurately can we predict offence categories?
:::

```{r libraries, include = FALSE}

library(ggplot2)
library(dplyr)
library(kknn)
library(tidyverse)
library(tidymodels)
library(knitr)
library(vip)
library(randomForest)

```

```{r loaddata, include = FALSE}
illinois_doc_data <- read.csv("../CSV Files/merged_data.csv")
cleaned_doc_data <- illinois_doc_data |> select(-c(last_name, name, id)) |>
  mutate(offense_category = as.factor(offense_category),
         adm_appx_age = appx_age,
         weight_lbs = as.numeric(weight),
         sex = as.factor(sex),
         height_in = as.numeric(height),
         race = as.factor(race),
         eye_color = as.factor(eyes),
         hair = as.factor(hair),
         class = as.factor(class),
         parent_institution = as.factor(parent_institution)) |>
  select(-c(eyes, height, weight, appx_age)) |> na.omit()
```

```{r ttsplit, include = FALSE}

set.seed(445)
df_split <- initial_split(cleaned_doc_data, prop = 0.8)
df_train <- training(df_split)
df_train
df_test <- testing(df_split)

```

## K-Nearest Neighbors Background

-   K-Nearest Neighbors (KNN) is a classification method that loops through each point in a dataset, and identifies the k points that are closest to that point

    -   This k value is adjustable and helps determine the classifier itself, so tuning k is critical for optimizing performance.

## KNN on Offense Category

-   **Goal:** Predict offense category for individuals based on all other variables.

    -   **Offense categories (manually created):** controlled substance possession without a prescription, burglary, murder, armed robbery, theft (identity or property), battery, sexual assault, forgery, kidnapping, illegal firearm/weapon/handgun use or possession, harassment, bribery, drug/meth manufacturing, vehicular hijacking/theft, DUI, child porn, obstructing justice, home invasion, and other

        -   Categories besides other accounted for \>90% of offenses

## Drawbacks/Issues

-   **Problem:** This method can be extremely time-expensive with large training sets due to the nature of looping through each point, as I found out during data fitting

    -   47,000+ training observations (for an 80-20 split)

    -   2.75 hour runtime

-   No variable selection or subset with KNN

## KNN Cross Validation Results

-   Ten-fold cross-validation performed with the following possible k-values:

    -   10, 50, 100, 200, 500, 1000, 5000

    -   Chose wide range due to large amount of data

-   Cross-validation used to pick the k-value returning the best accuracy, which was k=50

-   Accuracy decreased significantly for k \> 50

```{r knn_cv, fig.width = 8, fig.height = 6}

tune_results <- readRDS("../Code/knn_CV_results.rds")
autoplot(tune_results)

```

## KNN Model Performance

```{r knn_performance, fig.width = 2, fig.height = 2}

final_knn_fit <- readRDS("../Code/final_knn_model.rds")
knn_preds <- predict(final_knn_fit, new_data = df_test) |>
  bind_cols(df_test)
knn_conf_mat <- knn_preds |> conf_mat(truth = offense_category,
                                      estimate = .pred_class)
knn_conf_mat_df <- data.frame(knn_conf_mat$table)
knn_table <- knn_conf_mat_df |> group_by(Truth) |>
  summarize(total = sum(Freq),
            correct = sum(Freq[Truth == Prediction]),
            prop_correct = round(correct / total, 3))
knn_table1 <- knn_table[1:9,]
knn_table2 <- knn_table[10:17,]
kable(knn_table1)
kable(knn_table2)

```

## KNN Model Analysis

-   Test error rate seems high on the surface (almost 40%)
    -   But we have to account for the fact that 17 categories are present
        -   Expected guessing error w/ 17 prediction categories: \~94.12%
-   Predictive accuracy was significantly better in categories with more observations (e.g., battery, illegal weapon use/possession, murder) than categories with few observations (e.g., vehicular hijacking/theft, forgery).
    -   Only exceptions: drug manufacturing, theft (\< 45% accuracy).
-   Interpretation: more variability in predictor means for lower level offenses

## LASSO

::::: columns
::: column
### LASSO model = RSS + $\lambda \sum_{j = 1}^{p} \beta_{j}^2$

where $\lambda$ is the tuning parameter, to improve interpretability, use penalty to perform feature selection

-   cross validation selected $\lambda = 0.01$
:::

::: column
```{r}

lasso_terms <- read.csv("../Code/LinearRegressionOutputs/lasso_terms.csv")
kable(lasso_terms[,c("term", "estimate")])

```
:::
:::::

## Forward and Backward Subset

::::: columns
::: column
```{r}

vars <- read.csv("../Code/LinearRegressionOutputs/ranked_vars.csv")
merge_vars <- vars[1:8,c("step", "variable")]
merge_vars$"step (cont)" <- vars[9:16, "step"]
merge_vars$"variable (cont)" <- vars[9:16, "variable"]
kable(merge_vars)

```
:::

::: column
```{r}

rmse <- read.csv("../Code/LinearRegressionOutputs/rmse_v_terms.csv")

ggplot(rmse) + geom_point(aes(num_terms, RMSE, color = RMSE == min(RMSE))) + scale_color_discrete(labels = c("larger than optimum", "minimum RMSE")) + labs(x = "Number of Terms", color = "", caption = "dropped terms: num_scars, sex, num_other_marks, height")

```
:::
:::::

## Generalized Additive Model (GAM)

::: incremental
-   Extension of linear regression that allows each predictor to have a continuous nonlinear effect on the response

-   80:20 split

-   Used 10 fold CV to tune the degrees of freedom

-   On the test set the GAM achieved:

-   RMSE = 12.20631

-   Meaning that our prediction error is $12.20631$ years.

-   $R^2$=0.7139504

-   Meaning that our model explains 71% of the variability in sentencing length on new data.
:::

## GAM

```{r, echo=FALSE, fig.width=12, fig.height=8.25}

# gam_fit <- readRDS("~/project-5/Code/gam_fit.rds")
# sent_train <- readRDS("~/project-5/Code/GAM_train.rds")
# train_pred <- readRDS("~/project-5/Code/Gam_pred.rds")

gam_fit <- readRDS("../Code/gam_fit.rds")
sent_train <- readRDS("../Code/GAM_train.rds")
train_pred <- readRDS("../Code/gam_pred.rds")




pred_dat <- train_pred |>
  group_by(year_adm) |>
  summarise(
    .pred       = mean(.pred,       na.rm = TRUE),
    .pred_lower = mean(.pred_lower, na.rm = TRUE),
    .pred_upper = mean(.pred_upper, na.rm = TRUE),
    .groups     = "drop"
  )

out_dat <- sent_train |>
  select(year_adm, current_sentence)

ggplot() +
  geom_point(
    data = out_dat,
    aes(x = year_adm, y = current_sentence),
    alpha = 0.007
  ) +
  geom_line(
    data = pred_dat,
    aes(x = year_adm, y = .pred),
    color = "coral4"
  ) +
  geom_line(
    data = pred_dat,
    aes(x = year_adm, y = .pred_lower),
    color = "coral4",
    linetype = "dashed"
  ) +
  geom_line(
    data = pred_dat,
    aes(x = year_adm, y = .pred_upper),
    color = "coral4",
    linetype = "dashed"
  ) +
  coord_cartesian(xlim = c(1980, 2018)) +
  labs(
    x = "Year admitted",
    y = "Sentence length",
    title = "GAM effect for year admitted 1980â€“2018"
  ) +
  theme_bw(base_size = 18) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold"),
    axis.text  = element_text(size = 14)
  )

```

## Random Forest

::::: columns
::: column
-   Chosen to deal with correlation amongst trees.
-   58,836 observations. 70:30 Split $\sim$ (41,200 \| 17,600)
-   Fit 500 trees with 4 randomly chosen predictors for each tree.
-   Out-of-Bag error is calculated by checking the performance of each tree on a subset of unseen data during the training process.\
:::

::: column
```{r, echo=FALSE, fig.width=12, fig.height=8.25}

#You must use the download link to download the rds file locally and adjust the pathway then store it in rf_fit.
#
#rf_fit <- readRDS("~/Documents/RFfit_model_juan.rds")
rf_fit <- readRDS("../Code/RFfit_model_juan.rds")



vi <- vi(rf_fit$fit) %>%
  top_n(5, Importance) %>%
  mutate(Variable = recode(Variable,
                           year_adm = "Year Admitted",
                           parent_institution = "Parent Institution",
                           total_counts = "Total Counts",
                           time_sentenced_prior = "Prior Sentencing Time",
                           class = "Illinois Felony Class",
                           ))

vip(vi, aesthetics = list(fill = "coral4"))  +
  ggtitle("5 Most Important Predictors in our Random Forests Model") + theme_bw() + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text = element_text(size = 13),
        plot.title = element_text(size = 16, hjust = 0.5))



```
:::
:::::

## Predictive Performance - Random Forests

::::: columns
::: column
-   **12.11** OOB RMSE / **12. 18** Test RMSE. Similar RMSEs is good (generalizes well).
-   As sentencing length goes up, our models prediction becomes more variable. However, the models' errors scale proportionally to sentencing length. Meaning we don't typically get crazy predictions for lower sentencing lengths.
:::

::: column
```{r, echo=FALSE, fig.width=8}

rmses <- read_csv("../CSV Files/RF_RMSEbySentencing.csv")


ggplot(data = rmses, aes(x = current_sentence_rnd, y = rmse)) + 
  labs(title = "RMSE by Sentence Length", x = "Sentencing Time", y = "RMSE") +
  geom_smooth(method = "loess", se = FALSE, color = "coral4") + geom_line() +
  theme_bw()+theme(plot.title = element_text(hjust = 0.5))

```
:::
:::::

## Conclusion

-   The relationship between prison sentence and our predictors is likely not linear

    -   random forest performed best, followed by GAM, and then linear regression

-   The best predictors of prison sentence are year admitted, parent institution, total counts charged, prior sentencing time, and criminal class, and offense category

    -   demographic data like race, sex, and weight are less helpful predictors

## References

Fisher, David. 2019. Illinois DOC labeled faces dataset, Version 1. Retrieved 11/19/2025 from <https://www.kaggle.com/datasets/davidjfisher/illinois-doc-labeled-faces-dataset/data>.

Death Penalty Information Center. (2023). *Time on Death Row*. Death Penalty Information Center; Death Penalty Information Center. https://deathpenaltyinfo.org/death-row/death-row-time-on-death-row.

## Questions?

## Bonus Slides - Complete Variable Importance for RF

```{r, echo=FALSE, fig.width=8}

vi_all <- vi(rf_fit$fit) %>%
  top_n(20, Importance) #%>%
  # mutate(Variable = recode(Variable,
  #                          year_adm = "Year Admitted",
  #                          parent_institution = "Parent Institution",
  #                          total_counts = "Total Counts",
  #                          time_sentenced_prior = "Prior Sentencing Time",
  #                          class = "Illinois Felony Class",
  # ))

vip(vi_all, num_features = 20,aesthetics = list(fill = "coral4"))  +
  ggtitle("Variable Importance Plot for our Random Forests Model") + theme_bw()

```
