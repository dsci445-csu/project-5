---
title: "paper"
author: "Group 5"
date: "2025-12-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages <- c("lubridate", "tidyverse", "tidymodels", "rpart.plot", "vip", "Metrics", "knitr", "randomForest")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
remove(packages, pkg)

```

## Abstract

Incarceration and criminal justice are two large issues currently affecting
the United States.  Although there has been work to mend some of the past
injustices that have occurred on the basis of race, gender, sexual identity,
and more, bias and discrimination undoubtedly remain apart of our society.
In particular, one issue that typically exemplifies these unresolved issues
is the prison system. This report takes the approach of analyzing prison
specific data, from the Illinois Department of Corrections, using machine
learning to predict charge category, as well as sentencing length.  Predicting
offense category was much more successful for offense categories with a large
sample, potentially indicating more variability in predictors for lower-level
offenses. For sentence length, random forest modeling performed the best,
with the relationship between prison sentence and the predictor variables
likely not being linear.  Demographic data, such as race, sex, and weight
were less important predictors for sentencing length, indicating an
encouraging sign that sentence length is based primarily on criminal history
and the specific crime committed, rather than being biased by physical traits.

## Background
here's our data and why we chose it
here's where the data is from
here are our research questions

Our data comes from kaggle, it contains three csv files; sentencing, person, and marks. Across these datasets we have a unique id for each prisoner assigned by the Illinois Department of Corrections. (Talk about data cleaning and the work put into making one final dataframe)

### Research Questions
- What predictors have the strongest effect on sentencing length?
- How accurately can we predict sentencing length?
- How accurately can we predict offence categories?

## Methods
### KNN

Within the criminal justice system, it is expected that some patterns will
exist for sentencing type, prior offenses, counts and severity of a charge
(i.e., was it armed or aggravated), among other variables. However, there
are some patterns or correlations that would ideally not exist, in a perfect
criminal justice system (such as offense category hypothetically being 
correlated with number of scars).  With these seemingly conflicting 
ideas in mind, this section of analysis focuses on using
K-nearest neighbors (KNN) to attempt to predict offense category within the
data.

As such, in order to answer research question regarding 
prediction of offense category, we decided to fit a KNN model. KNN stands
for k-nearest neighbor, which is an algorithm that cycles through each point
in a dataset, and identifies the k points that are closest to that point.
This k value is adjustable and helps determine the classifier itself, so tuning
k is critical for optimizing classification performance.
I grouped offenses/charges into seventeen categories: controlled subst poss w/out 
prescription, burglary, murder, armed robbery, theft (identity or property),
battery, sexual assault, forgery, kidnapping,
illegal firearm/weapon/handgun use or possession, harassment, bribery,
drug/meth manufacturing, vehicular hijacking/theft, DUI, child porn,
obstructing justice, home invasion, and other. It is important to note that
these categories were created by reading through thousands of different
individual charges, all unique and specific to each person. As such, these
categories were the result of wide generalizations, and in no way serve as
definite classifiers of charge or individual offense.  With that said,
the eighteen categories (besides other) captured a little more than 90 percent
of all observations, meaning the vast majority of charges fell into one of the
categories listed above.

Because this dataset is so large (even with an 80-20 training-test split), ten
fold cross validation (which is the process of partitioning the entire data
set in ten different folds, where each fold is the test set one time and the
other nine folds are used for training) was extremely time-expensive (about
2 hours and 45 minutes to complete).  An additional drawback to this approach
is the inability to perform variable selection ahead of time, as can be done
using best subset selection in various types of linear regression. Still, the
use of all variables in the model will make it easier to interpret which
variables were the most impactful in model performance in the context of all
possible predictors.

The first step of building the model was to load the data, clean the predictors
by ensuring all variables were correctly factors or numeric, and omit rows
with NA values after transforming the necessary columns. Afterwards, the data
was randomly split into an 80-20 training-test set.  Then, a range of k-values
to be used in tuning was selected. Because time-cost was so expensive for adding
additional k-values to be trained, only seven were selected: 10, 50, 100, 200,
500, 1000, and 5000.  The wide variety of k values was selected due to the
size of the data, and anticipated complexity.  A larger range allows for
comparison of model performance with a greater variety of k-values selected
for KNN.  After this, ten folds for cross validation were fit, and the recipe
of model fit (`offense_category` explained by all other variables) as well as
the `tidymodels` spec was set for KNN classification. Finally, cross-validation
was performed to find the k-value with the highest accuracy in prediction.
```{r KNN_Prep, echo = FALSE}
set.seed(445)
illinois_doc_data <- read.csv("../CSV Files/merged_data.csv")
cleaned_doc_data <- illinois_doc_data |> select(-c(last_name, name, id)) |>
  mutate(offense_category = as.factor(offense_category),
         adm_appx_age = appx_age,
         weight_lbs = as.numeric(weight),
         sex = as.factor(sex),
         height_in = as.numeric(height),
         race = as.factor(race),
         eye_color = as.factor(eyes),
         hair = as.factor(hair),
         class = as.factor(class),
         parent_institution = as.factor(parent_institution)) |>
  select(-c(eyes, height, weight, appx_age)) |> na.omit()
head(cleaned_doc_data)
df_split <- initial_split(cleaned_doc_data, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
```

After reading in the cross-validation results, we can plot the variation in
accuracy for each $k$ value examined.
```{r KNN_CV_Results, echo = FALSE}
knn_tune_results <- readRDS("~/DSCI445/project-5/Code/knn_CV_results.rds")
autoplot(knn_tune_results)
```
Immediately, it is clear that predictive accuracy decreases substantially as
k increases, with the best accuracy occurring when $k = 50$. Using this best
model obtained through cross-validation, the next step of the analysis was to
finalize the best model, and perform predictions on the test set.
```{r KNN_preds, echo = FALSE}
final_knn_fit <- readRDS("~/DSCI445/project-5/Code/final_knn_model.rds")
knn_preds <- predict(final_knn_fit, new_data = df_test) |>
  bind_cols(df_test)
knn_conf_mat <- knn_preds |> conf_mat(truth = offense_category,
                                      estimate = .pred_class)
knn_conf_mat_df <- data.frame(knn_conf_mat$table)
knn_table <- knn_conf_mat_df |> group_by(Truth) |>
  summarize(total = sum(Freq),
            correct = sum(Freq[Truth == Prediction]),
            prop_correct = round(correct / total, 3))
kable(knn_table, caption = "KNN Classification Recall")
```
These test set prediction results indicate a clear trend: total proportion
correctly predicted is significantly higher in categories with more individuals.
Logically, this makes sense because the model attempts to maximize accuracy by
predicting the best within categories with more samples. For example, 
individuals marked as having a "sexual offense" were correctly predicted 72 
percent of the time,  those categorized for murder were correctly predicted
91.4 percent of the time, and those charged with illegal possession of a
controlled substance were correctly classified 76.9 percent of the time.  These
classification rates are impressive, considering the number of categories that
each individual could be predicted as, indicating similarlity of predictor 
values (means) for individuals in these categories. The only categories
with relatively significant sample sizes and poor predictive accuracy were
burglary, theft, and drug manufacturing.  A potential explanation for this
poorer predictive performance would be greater variation in predictor variable
values for these "lower-level" crimes (which might be classified as lower-level
felonies or misdemeanors than murder or a sexual crime). Overall, this KNN model
performed relatively well, achieving an overall accuracy of roughly 62.3 percent.
Given that randomly classifying each individual into an offense category would
result in an expected accuracy around six percent (1/17), this is a significant
improvement, and demonstrates predictor mean similarity between individuals
charged with the same categories of crimes (particularly for more severe
charges).

### Random Forest

We made some decisions for this model, filtering out people with the hair color "Sandy" as there were very few observations (5). We also decided to not predict for people who were sentenced to zero years, as there were too many data entry issues around these rows which made no sense. This still left us with a large dataset at 59388 observations, where we decided random foresting would be a good model to run our data through for predicting a prisoner's sentencing time in years.The use of random forests allows for several trees to be fit with random subsets of our data and then combines results to better assess predictions. We found random foresting to be a good option because of its built-in out-of-bag feature which allows for us to run the model on all predictors and not worry about cross-validation. We didn't want the correlated trees you would expect in boosting, random forests don't care for the most important features it learns from tree to tree. It builds its trees on random subsets of predictors each time, while boosting begins to "boost" predictions by further finding optimal splits with the important features it learns as it goes. We went with 500 trees, and 4 predictors for each tree because it's common practice to choose the square root of how many predictors you have which is $\sqrt{20} = 4.47$. It's also important we note that we kept a similar training split for this model at 70:30 ( $\sim$ 17 41,200:17,600 ). Once this model was fit, we were eager to look into the feature importance plot to see if it was any different to our previous models.  

```{r, warning=FALSE, echo=FALSE, cache=TRUE, fig.width=7, fig.height=4, dpi=300, out.width="70%", fig.align='center'}  

df_load <- read.csv("../CSV Files/merged_data.csv")


df <- df_load %>% select(-id, -last_name, -name, -life_sentence, -death_sentence)%>%
  mutate(time_sentenced_prior = round(time_sentenced_prior,2),
         height = as.numeric(height),
         weight = as.numeric(weight)) %>% filter(!hair %in% c("Sandy", "Not Available"),
                                                 !eyes %in% c("Not Available"), current_sentence > 0)

df <- na.omit(df)

df <- df %>%
  mutate(across(where(is.character), as.factor),
         across(where(is.logical), ~factor(., levels = c(FALSE, TRUE))))

df_split <- initial_split(df, prop = 0.7)

df_train <- training(df_split)
df_test <- testing(df_split)

#rf_fit <- readRDS("~/Documents/RFfit_model_juan.rds")
rf_fit <- readRDS("RFfit_model_juan.rds")

vi <- vi(rf_fit$fit) %>%
  top_n(5, Importance) %>%
  mutate(Variable = recode(Variable,
                           year_adm = "Year Admitted",
                           parent_institution = "Parent Institution",
                           total_counts = "Total Counts",
                           time_sentenced_prior = "Prior Sentencing Time",
                           class = "Illinois Felony Class",
                           ))

vip(vi, aesthetics = list(fill = "coral4"))  +
  ggtitle("5 Most Important Predictors in our Random Forests Model") + theme_bw() + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text = element_text(size = 11),
        plot.title = element_text(size = 12, hjust = 0.5))

```

The predictor with the most importance in this case is the year the prisoner was admitted, which instantly tells us based off our model that the time period they were sentenced plays the biggest role in sentence length. It's interesting to think about and hypothesize why this may be; Has the way certain charges have been punished drastically changed throughout the years? Another predictor that stood out from this variable importance plot was total counts, which makes sense looking at it from a judge's point of view. It makes sense that a person with more charges historically will often get a longer sentence.

We checked model performance first by looking at the out-of-bag RMSE which measures prediction error on the training split by evaluating each tree on the subset of data not used during its training. We got a value of 12.11, which up to this point was the best, but still not ideal at first glance. We also looked at the test RMSE which checks the model performance on the test split and we found a very similar value at 12.18, meaning our model generalized well. We took a further step to look into why our predictions were off that much by grouping by sentencing length predicted and then calculating RMSE per sentencing length. The motivation here was that we feared our model was no good if we were off by 12 years for people with sentencing lengths of 1-5 years. As you can see below, we were very happy to find that this was not the case. We don't get into that 12 RMSE region until just about 50 predicted sentencing years. With such a broad range from 1-100 on our response variable, I would actually consider our RMSE not that bad with all things considered.

```{r, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE, fig.width=7, fig.height=4, dpi=300, out.width="70%", fig.align='center'}

rmses <- read_csv("../CSV Files/RF_RMSEbySentencing.csv")
ggplot(data = rmses, aes(x = current_sentence_rnd, y = rmse)) + 
  labs(title = "RMSE by Sentence Length", x = "Sentencing Time", y = "RMSE") +
  geom_smooth(method = "loess", se = FALSE, color = "coral4") + geom_line() +
  theme_bw()+theme(plot.title = element_text(hjust = 0.5))
```


### Linear Regression
To explore the relationship between prison sentence and our other variables, we chose to analyze a variety of linear models for their superior interpretability over non-parametric models. Because we were interested in identifying the most important variables for prison sentence prediction, we chose not to fit either an ordinary least squares regression model or a Ridge regression model. Both of these models would have assigned a non-zero coefficient to every predictor in the model, requiring every variable to be incorporated into the interpretation of a given prison sentence. We suspected that not every variable would be important so we focused on models that have a component of feature selection, including LASSO and subset selection models.

The first model we fit was the LASSO model, which selects optimal predictor coefficients by minimizing the combination of residual sum of squares and a penalty shrinking the size of those coefficients according to the formula below. 

$$
\sum_{i=1}^n (y_i-\beta_0-\sum_{j=1}^p\beta_jx_j)^2+\lambda\sum_{j=1}^p|\beta_j|=\text{RSS } + \lambda\sum_{j=1}^p|\beta_j|
$$
The penalty parameter $\lambda$ was chosen according to 10 fold cross validation using 100 possible values of $\lambda$ between 0.01 and $10 \times 10^10$. As shown on the lambda vs. RMSE graph, cross validation selected $\lambda = 0.01$.
```{r, echo = F, out.width="50%"}
include_graphics("LinearRegressionOutputs/lasso.png")
```

Unfortunately, due to the number of categorical predictors in our dataset, LASSO was not as helpful as we would have liked. Categorical predictors must be reference encoded for LASSO to compute coefficients, turning our 20 predictors into 88. Of these 88 terms, LASSO only selected out the following five variables: 

```{r, echo = F}
lasso_terms <- read.csv("LinearRegressionOutputs/lasso_terms.csv")
kable(lasso_terms[,c("term", "estimate")])
```

This tells us that height is not a significant predictor of prison sentence length. The other four variables, however, are individual categories from hair color, sex, and race respectively. These do not encapsulate whole categorical variables, so this information is not meaningfully interpretable, as it is not possible to only drop some categories from a categorical variable in linear regression. 

Since LASSO did not produce as interprative a model as we had hoped, we moved on to subset selection. We elected not to perform best subset selection, as this method of subset selection requires fitting a model with every possible combination of variables and we decided that fitting approximately $2^20$ models would be too computationally expensive. Instead we fit both forward and backward subset selection. Using the metric of adjusted $R^2$, forward and backward subset selection selected the same combination of variables, excluding the number of scars an individual has, their sex, number of other marks, and height, and including the following variables in order of importance.
```{r, echo = F}
vars <- read.csv("../Code/LinearRegressionOutputs/ranked_vars.csv")
merge_vars <- vars[1:8,c("step", "variable")]
merge_vars$"step (cont)" <- vars[9:16, "step"]
merge_vars$"variable (cont)" <- vars[9:16, "variable"]
kable(merge_vars)
```


```{r}
rmse <- read.csv("LinearRegressionOutputs/rmse_v_terms.csv")

ggplot(rmse) + geom_point(aes(num_terms, RMSE, color = RMSE == min(RMSE))) + scale_color_discrete(labels = c("larger than optimum", "minimum RMSE")) + labs(x = "Number of Terms", color = "", caption = "dropped terms: num_scars, sex, num_other_marks, height")
```


if the relationship is approximately linear, least squares will have low bias
if we have many data points, n >> p, also have low variance -- do have many data points
limitations to linear model: linearity is an approximation

use LASSO, forward, backward to obtain more interpretable model, drop predictors not associated with the response

not doing best subset because there are over 20 predictors and fitting approximately $2^20$ models would be too computationally expensive; not doing principle component regression because interpretability again; ridge doesn't eliminate does not eliminate predictors and I care about interpretation

Ridge regression: RSS + $\lambda \sum_{j = 1}^{p} \beta_{j}^2$ where $\lambda$ is the tuning parameter
- variable selection
- LASSO
- Ridge
- results

### GAM
We decided to use fit a generalized additive model(GAM) to see how well it can predict sentencing length and what predictors have the strongest effect on sentencing length. GAMs are an extension of linear regression that are used to show relationships between the predictors that can be better explained non-linearly while being easily interpretable.  We then used 10 fold Cross-Validation to evaluate the model 

When fitting a model that includes physical descriptors and a model without any of the physical descriptors. The model that was able to best predict sentencing length was:







This model shows that:


- results

## Conclusion
here's why we pick x model over y model

## References
kaggle data set
