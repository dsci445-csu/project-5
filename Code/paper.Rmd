---
title: "paper"
author: "Group 5"
date: "2025-12-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages <- c("lubridate", "tidyverse", "tidymodels", "rpart.plot", "vip", "Metrics", "knitr", "randomForest")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
remove(packages, pkg)

```

## Abstract
have a summary of the paper

## Background

We chose to analyze a dataset recording people incarcerated in Illinois because we were curious to understand the relationships underlying an individual's prison sentence. Our data comes from kaggle, it contains three csv files; sentencing, person, and marks. Across these datasets we have a unique id for each prisoner assigned by the Illinois Department of Corrections. Because this data was scraped from the Illinois Department of Corections' website, this data required a substantial amount of cleaning. The variables in these datasets were initially separated by semicolons and the data failed to parse into designated columns because there were also semicolons used within the information scraped from the website. A parser was used to replace all within-text semicolons with commas to resolve this issue. The sentencing and marks files had different rows for each of an individual's charges and marks respectively. Thus these files were modified to group together all of the individual's rows so that each row had a unique ID and the three datasets could be merged together on ID. Within the sentencing csv, individuals recieved a sentence that was either numerical, indicating the length of time they were sentenced to serve, or one of the following categories: life, death, or sexually dangerous person. The sentence of sexually dangerous person did not lead to time served in prison but did lead to that individual being placed on a registry for sexually dangerous persons, so during data cleaning this category was replaced with the numerical value 0. Individuals who received a death sentence were assigned a prison sentence of 15 years, as our research indicated this was the median amount of time an individual spent in prison while waiting for a death sentence to be enacted. More severe prison sentences were represented in the sentencing csv in a variety of ways. Some individuals received sentences with lengths of time longer than the human lifespan, including some sentences greater than 3,000 years. To standardize these numbers, prison sentences were capped at 100 years in the data cleaning process. Those who received a life sentence were also assigned a numerical sentence of 100 years to match this standardization.

### Research Questions
- What predictors have the strongest effect on sentencing length?
- How accurately can we predict sentencing length?
- What predictors have the strongest effect on offence category?
- How accurately can we predict offence categories?

To investigate these research questions, we fit four different categories of models. Linear models, generalized additive models, and tree-based models were used to investigate sentencing length. K nearest neighbors based models were used to investigate offense category. 

## Methods
### KNN

Within the criminal justice system, it is expected that some patterns will
exist for sentencing type, prior offenses, counts and severity of a charge
(i.e., was it armed or aggravated), among other variables. However, there
are some patterns or correlations that would ideally not exist, in a perfect
criminal justice system (such as offense category hypothetically being 
correlated with number of scars).  With these seemingly conflicting 
ideas in mind, this section of analysis focuses on using
K-nearest neighbors (KNN) to attempt to predict offense category within the
data.  While predictive accuracy is one goal, the expectations raised in this
section also indicate a desire to examine variable importance within prediction,
to see if expected or unexpected variables are having large impacts on
predictive accuracy in the model.

As such, in order to answer our second two research questions regarding 
prediction of offense category, as well as determine which predictors have the
strongest effect on offense category, we decided to fit a KNN model. KNN stands
for k-nearest neighbor, which is an algorithm that cycles through each point
in a dataset, and identifies the k points that are closest to that point.
This k value is adjustable and helps determine the classifier itself, so tuning
k is critical for optimizing classification performance.
I grouped offenses/charges into nineteen categories: controlled subst poss w/out 
prescription, burglary, murder, armed robbery, theft (identity or property),
battery, sexual assault, forgery, kidnapping,
illegal firearm/weapon/handgun use or possession, harassment, bribery,
drug/meth manufacturing, vehicular hijacking/theft, DUI, child porn,
obstructing justice, home invasion, and other. It is important to note that
these categories were created by reading through thousands of different
individual charges, all unique and specific to each person. As such, these
categories were the result of wide generalizations, and in no way serve as
definite classifiers of charge or individual offense.  With that said,
the eighteen categories (besides other) captured a little more than 90 percent
of all observations, meaning the vast majority of charges fell into one of the
categories listed above.

Because this dataset is so large (even with an 80-20 training-test split), ten
fold cross validation (which is the process of partitioning the entire data
set in ten different folds, where each fold is the test set one time and the
other nine folds are used for training) was extremely time-expensive (about
2 hours and 45 minutes to complete).
```{r, warning = FALSE}
set.seed(445)
illinois_doc_data <- read.csv("../CSV Files/merged_data.csv")
cleaned_doc_data <- illinois_doc_data |> select(-c(last_name, name, id)) |>
  mutate(offense_category = as.factor(offense_category),
         adm_appx_age = appx_age,
         weight_lbs = as.numeric(weight),
         sex = as.factor(sex),
         height_in = as.numeric(height),
         race = as.factor(race),
         eye_color = as.factor(eyes),
         hair = as.factor(hair),
         class = as.factor(class),
         parent_institution = as.factor(parent_institution)) |>
  select(-c(eyes, height, weight, appx_age)) |> na.omit()
head(cleaned_doc_data)
df_split <- initial_split(cleaned_doc_data, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)

k_vals <- expand.grid(neighbors = c(10, 50, 100, 200, 500, 1000,
                                     2000, 5000, 10000))
knn_10foldcv <- vfold_cv(df_train, v = 10)
knn_spec <- nearest_neighbor(mode = "classification", neighbors = tune()) |>
  set_engine("kknn")
knn_rec <- recipe(offense_category ~ ., data = df_train) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors())
knn_wf <- workflow() |> add_recipe(knn_rec) |> add_model(knn_spec)
#tune_results <- tune_grid(knn_wf,
#                          resamples = knn_10foldcv,
#                          grid = k_vals)
#collect_metrics(tune_results)
#best_knn <- select_best(tune_results, metric = "accuracy")
#final_knn <- finalize_workflow(knn_wf, best_knn)
#final_knn_fit <- fit(final_knn, data = df_train)
#end_time <- Sys.time()
#elapsed <- end_time - start_time
#elapsed
# 2. Create a confusion matrix for the `test` data
#knn_preds <- predict(knn_fit, new_data = df_test)

#knn_test_res |>
#  conf_mat(truth = offense_category, estimate = .pred_class)

# test error
#test_res |>
 # accuracy(truth = offense_category, estimate = .pred_class) |>
#  mutate(error = 1 - .estimate) |>
#  pull(error)


```


### Random Forest

We made some decisions for this model, filtering out people with the hair color "Sandy" as there were very few observations (5). We also decided to not predict for people who were sentenced to zero years, as there were too many data entry issues around these rows which made no sense. This still left us with a large dataset at 59388 observations, where we decided random foresting would be a good model to run our data through for predicting a prisoner's sentencing time in years.The use of random forests allows for several trees to be fit with random subsets of our data and then combines results to better assess predictions. We found random foresting to be a good option because of its built-in out-of-bag feature which allows for us to run the model on all predictors and not worry about cross-validation. We didn't want the correlated trees you would expect in boosting, random forests don't care for the most important features it learns from tree to tree. It builds its trees on random subsets of predictors each time, while boosting begins to "boost" predictions by further finding optimal splits with the important features it learns as it goes. We went with 500 trees, and 4 predictors for each tree because it's common practice to choose the square root of how many predictors you have which is $\sqrt{20} = 4.47$. It's also important we note that we kept a similar training split for this model at 70:30 ( $\sim$ 17 41,200:17,600 ). Once this model was fit, we were eager to look into the feature importance plot to see if it was any different to our previous models.  

```{r, warning=FALSE, echo=FALSE, cache=TRUE, fig.width=7, fig.height=4, dpi=300, out.width="70%", fig.align='center'}  

df_load <- read.csv("../CSV Files/merged_data.csv")


df <- df_load %>% select(-id, -last_name, -name, -life_sentence, -death_sentence)%>%
  mutate(time_sentenced_prior = round(time_sentenced_prior,2),
         height = as.numeric(height),
         weight = as.numeric(weight)) %>% filter(!hair %in% c("Sandy", "Not Available"),
                                                 !eyes %in% c("Not Available"), current_sentence > 0)

df <- na.omit(df)

df <- df %>%
  mutate(across(where(is.character), as.factor),
         across(where(is.logical), ~factor(., levels = c(FALSE, TRUE))))

df_split <- initial_split(df, prop = 0.7)

df_train <- training(df_split)
df_test <- testing(df_split)

#rf_fit <- readRDS("~/Documents/RFfit_model_juan.rds")
rf_fit <- readRDS("RFfit_model_juan.rds")

vi <- vi(rf_fit$fit) %>%
  top_n(5, Importance) %>%
  mutate(Variable = recode(Variable,
                           year_adm = "Year Admitted",
                           parent_institution = "Parent Institution",
                           total_counts = "Total Counts",
                           time_sentenced_prior = "Prior Sentencing Time",
                           class = "Illinois Felony Class",
                           ))

vip(vi, aesthetics = list(fill = "coral4"))  +
  ggtitle("5 Most Important Predictors in our Random Forests Model") + theme_bw() + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text = element_text(size = 11),
        plot.title = element_text(size = 12, hjust = 0.5))

```

The predictor with the most importance in this case is the year the prisoner was admitted, which instantly tells us based off our model that the time period they were sentenced plays the biggest role in sentence length. It's interesting to think about and hypothesize why this may be; Has the way certain charges have been punished drastically changed throughout the years? Another predictor that stood out from this variable importance plot was total counts, which makes sense looking at it from a judge's point of view. It makes sense that a person with more charges historically will often get a longer sentence.

We checked model performance first by looking at the out-of-bag RMSE which measures prediction error on the training split by evaluating each tree on the subset of data not used during its training. We got a value of 12.11, which up to this point was the best, but still not ideal at first glance. We also looked at the test RMSE which checks the model performance on the test split and we found a very similar value at 12.18, meaning our model generalized well. We took a further step to look into why our predictions were off that much by grouping by sentencing length predicted and then calculating RMSE per sentencing length. The motivation here was that we feared our model was no good if we were off by 12 years for people with sentencing lengths of 1-5 years. As you can see below, we were very happy to find that this was not the case. We don't get into that 12 RMSE region until just about 50 predicted sentencing years. With such a broad range from 1-100 on our response variable, I would actually consider our RMSE not that bad with all things considered.

```{r, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE, fig.width=7, fig.height=4, dpi=300, out.width="70%", fig.align='center'}

rmses <- read_csv("../CSV Files/RF_RMSEbySentencing.csv")
ggplot(data = rmses, aes(x = current_sentence_rnd, y = rmse)) + 
  labs(title = "RMSE by Sentence Length", x = "Sentencing Time", y = "RMSE") +
  geom_smooth(method = "loess", se = FALSE, color = "coral4") + geom_line() +
  theme_bw()+theme(plot.title = element_text(hjust = 0.5))
```


### Linear Regression
To explore the relationship between prison sentence and our other variables, we chose to analyze a variety of linear models for their superior interpretability over non-parametric models. Because we were interested in identifying the most important variables for prison sentence prediction, we chose not to fit either an ordinary least squares regression model or a Ridge regression model. Both of these models would have assigned a non-zero coefficient to every predictor in the model, requiring every variable to be incorporated into the interpretation of a given prison sentence. We suspected that not every variable would be important so we focused on models that have a component of feature selection, including LASSO and subset selection models.

The first model we fit was the LASSO model, which selects optimal predictor coefficients by minimizing the combination of residual sum of squares and a penalty shrinking the size of those coefficients according to the formula below. 

$$
\sum_{i=1}^n (y_i-\beta_0-\sum_{j=1}^p\beta_jx_j)^2+\lambda\sum_{j=1}^p|\beta_j|=\text{RSS } + \lambda\sum_{j=1}^p|\beta_j|
$$
The penalty parameter $\lambda$ was chosen according to 10 fold cross validation using 100 possible values of $\lambda$ between 0.01 and $10 \times 10^10$. As shown on the lambda vs. RMSE graph, cross validation selected $\lambda = 0.01$.
```{r, echo = F, out.width="50%"}
include_graphics("LinearRegressionOutputs/lasso.png")
```

Unfortunately, due to the number of categorical predictors in our dataset, LASSO was not as helpful as we would have liked. Categorical predictors must be reference encoded for LASSO to compute coefficients, turning our 20 predictors into 88. Of these 88 terms, LASSO only selected out the following five variables: 

```{r, echo = F}
lasso_terms <- read.csv("LinearRegressionOutputs/lasso_terms.csv")
kable(lasso_terms[,c("term", "estimate")])
```

This tells us that height is not a significant predictor of prison sentence length. The other four variables, however, are individual categories from hair color, sex, and race respectively. These do not encapsulate whole categorical variables, so this information is not meaningfully interpretable, as it is not possible to only drop some categories from a categorical variable in linear regression. 

Since LASSO did not produce as interprative a model as we had hoped, we moved on to subset selection. We elected not to perform best subset selection, as this method of subset selection requires fitting a model with every possible combination of variables and we decided that fitting approximately $2^20$ models would be too computationally expensive. Instead we fit both forward and backward subset selection. Using the metric of adjusted $R^2$, forward and backward subset selection selected the same combination of variables, excluding the number of scars an individual has, their sex, number of other marks, and height, and including the following variables in order of importance.
```{r, echo = F}
vars <- read.csv("../Code/LinearRegressionOutputs/ranked_vars.csv")
merge_vars <- vars[1:8,c("step", "variable")]
merge_vars$"step (cont)" <- vars[9:16, "step"]
merge_vars$"variable (cont)" <- vars[9:16, "variable"]
kable(merge_vars)
```

Although both forward and backward subset selection indicated that the best model includes the above sixteen categories, an exploration indicates that smaller models can be fit with a minimal increase in RMSE, when the model is fit with a number of categories between five and fifteen. The graph below shows the RMSE for a model fit with the most important variable, followed by a model with the two most important variables, and so on up to sixteen variables, according to the ranking provided by the forward subset shown in the table above. This indicates that if interpretability is a priority, a model can be fit with five or six terms that will have nearly identical prediction results to the model with sixteen, but fewer variables would need to be explained. 

```{r, echo = F}
rmse <- read.csv("LinearRegressionOutputs/rmse_v_terms.csv")

ggplot(rmse) + geom_point(aes(num_terms, RMSE, color = RMSE == min(RMSE))) + scale_color_discrete(labels = c("larger than optimum", "minimum RMSE")) + labs(x = "Number of Terms", color = "", caption = "dropped terms: num_scars, sex, num_other_marks, height")
```

### GAM
We decided to use fit a generalized additive model(GAM) to see how well it can predict sentencing length and what predictors have the strongest effect on sentencing length. GAMs are an extension of linear regression that are used to show relationships between the predictors that can be better explained non-linearly while being easily interpretable.  We then used 10 fold Cross-Validation to evaluate the model 

When fitting a model that includes physical descriptors and a model without any of the physical descriptors. The model that was able to best predict sentencing length was:







This model shows that:


- results

## Conclusion
here's why we pick x model over y model

## References
kaggle data set
