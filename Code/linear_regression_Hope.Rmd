---
title: "linear_regression_Hope"
output: pdf_document
---
```{r, message = F}
packages <- c("ggplot2", "tidyverse", "tidymodels", "olsrr")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
remove(packages, pkg)

set.seed(445)
```

```{r}
# read in data
all_data <- read.csv("~/DSCI445/project-5/CSV Files/merged_data.csv")
str(all_data)
```

```{r prep data}
# drop id and name columns in dataset for prediction
useful_columns <- all_data |> subset(select = -c(id, last_name, name))
useful_columns$weight <- as.numeric(useful_columns$weight)
useful_columns$height <- as.numeric(useful_columns$height)
useful_columns$attempted <- as.numeric(useful_columns$attempted)
useful_columns$aggravated <- as.numeric(useful_columns$aggravated)
useful_columns$armed <- as.numeric(useful_columns$armed)
useful_columns$life_sentence <- as.numeric(useful_columns$life_sentence)
useful_columns$death_sentence <- as.numeric(useful_columns$death_sentence)

useful_columns |> drop_na() -> useful_columns
# convert categorical variables to dummy variables and normalize numerical variables
prep_data <- recipe(current_sentence ~ ., data = useful_columns) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())
```

```{r set up tune}
# set up lasso tuning
cv_10fold <- vfold_cv(useful_columns, v = 10)

lasso_spec <- linear_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("regression") |>
  set_engine("glmnet")

lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)
```

```{r tune}
set.seed(445)
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = cv_10fold, grid = tune_df) -> lasso_tune
```

```{r}
lasso_tune |>
  collect_metrics() |>
  select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, rmse^2)) +
  geom_point(aes(lambda, rmse^2)) +
  coord_trans(x = "log10")

## best penalty
show_best(lasso_tune, metric = "rsq", n = 1)

save_rs <- rep(0, 4)
save_rs[1] <- show_best(lasso_tune, metric = "rsq", n = 1)$mean
```


```{r}
# tune chose smallest provided value for lambda; verify this is the best by providing smaller penalties
tune_df$lambda <- 10^seq(-5, -2, length.out = 100)

set.seed(445)
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = cv_10fold, grid = tune_df) -> lasso_tune

## best penalty
show_best(lasso_tune, metric = "rsq", n = 1)
save_rs[2] <- show_best(lasso_tune, metric = "rsq", n = 1)$mean
```

```{r}
set.seed(445)
model <- lm(current_sentence ~ ., data = useful_columns)
backward <- ols_step_backward_adj_r2(model)
backward$metrics$adj_r2
save_rs[3] <- max(backward$metrics$r2)
```

```{r}
set.seed(445)
forward <- ols_step_forward_adj_r2(model)
save_rs[4] <- max(forward$metrics$r2)
```

```{r}
which.max(save_rs)
```
Backwards subset selection has the highest $R^2$ and so should be used for prediction.



