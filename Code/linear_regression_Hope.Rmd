---
title: "linear_regression_Hope"
output: pdf_document
---
```{r, message = F}
packages <- c("ggplot2", "tidyverse", "tidymodels", "olsrr", "glmnet")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
remove(packages, pkg)

set.seed(445)
```

```{r}
# read in data
all_data <- read.csv("../CSV Files/merged_data.csv")
str(all_data)
```

```{r prep data}
# drop id and name columns in dataset for prediction
useful_columns <- all_data |> subset(select = -c(id, last_name, name, life_sentence, death_sentence))
useful_columns$weight <- as.numeric(useful_columns$weight)
useful_columns$height <- as.numeric(useful_columns$height)
useful_columns$attempted <- as.numeric(useful_columns$attempted)
useful_columns$aggravated <- as.numeric(useful_columns$aggravated)
useful_columns$armed <- as.numeric(useful_columns$armed)

useful_columns |> drop_na() -> useful_columns
# convert categorical variables to dummy variables and normalize numerical variables
prep_data <- recipe(current_sentence ~ ., data = useful_columns) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())
```

```{r set up tune}
# set up lasso tuning
set.seed(445)
cv_10fold <- vfold_cv(useful_columns, v = 10)

lasso_spec <- linear_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("regression") |>
  set_engine("glmnet")

lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)
```

```{r tune}
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = cv_10fold, grid = tune_df) -> lasso_tune
```

```{r}
lasso_tune |>
  collect_metrics() |>
  select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, rmse)) +
  geom_point(aes(lambda, rmse)) +
  coord_trans(x = "log10") +
  labs(title = "LASSO Cross Validation", y = "RMSE") -> lasso_rmse


## best penalty
show_best(lasso_tune, metric = "rmse", n = 1)

save_rs <- rep(0, 4)
save_rs[1] <- show_best(lasso_tune, metric = "rsq", n = 1)$mean
```

```{r}
png("LinearRegressionOutputs/lasso.png")
lasso_rmse
dev.off()
```


```{r}
set.seed(445)
df_split <- initial_split(useful_columns, prop = 0.7)

df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r}
lasso_spec <- linear_reg(mixture = 1, penalty = show_best(lasso_tune, metric = "rmse", n = 1)$lambda) |>
    set_mode("regression") |>
    set_engine("glmnet")
  
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  fit(df_train) -> lasso

lasso |> tidy() -> lasso_df
lasso_df[lasso_df$estimate == 0,]
write.csv(lasso_df[lasso_df$estimate == 0, ], "LinearRegressionOutputs/lasso_terms.csv", row.names = F)
```
LASSO only sets 5 out of 88 terms to 0, and the terms set to 0 do not encapsulate whole categorical variables, so this information is not meaningfully interpretable. 

```{r}
set.seed(445)
model <- lm(current_sentence ~ ., data = useful_columns)
backward <- ols_step_backward_adj_r2(model)
backward$metrics
backward$metrics$adj_r2
```

```{r}
set.seed(445)
forward <- ols_step_forward_adj_r2(model)
forward$metrics
```

```{r}
renumbered <- backward$metrics
renumbered$step <- c(20, 19, 18, 17)
renumbered <- rbind(forward$metrics, renumbered)
renumbered <- renumbered[order(renumbered$step),]
write.csv(renumbered, "LinearRegressionOutputs/ranked_vars.csv", row.names = F)
```


```{r}
get_rmse <- function(model) {
  augment(model, new_data = df_test) |>
    mutate(resid2 = .resid^2) |>
    summarise(rmse = sqrt(mean(resid2))) |>
    pull(rmse)
}
```
Backwards subset selection has the highest $R^2$ and so should be used for prediction.

```{r}
lm_spec <- linear_reg()
df_backwards <- df_train |> select(-all_of(backward$metrics$variable))

backwards_recipe <- recipe(current_sentence ~ ., data = df_backwards)
backwards <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit(df_train)

rmse_backwards <- get_rmse(backwards)
rmse_backwards
```

```{r}
# drops num_other_marks, height, sex
# reverse order num_scars, month_adm, hair, death_sentence, attempted, weight, eyes, armed, num_tattoos -- 10
# race, appx_age, aggravated, time_sentenced prior, offense_category
# top 5 in order: year_adm, life_sentence, class, parent_institution, total_counts
```

```{r}
ranked_variables <- forward$metrics$variable
num_terms <- 1:length(ranked_variables)
rmse_vals <- rep(0, length(num_terms))

for (i in num_terms){
  df_subset <- useful_columns |> subset(select = c(ranked_variables[1:i], "current_sentence"))
  subset_recipe <- recipe(current_sentence ~ ., data = df_subset)
  subset_fit <- workflow() |> add_model(lm_spec) |> add_recipe(subset_recipe) |> fit(df_train)
  
  rmse_vals[i] <- get_rmse(subset_fit)
}

rmse_v_terms_df <- data.frame(num_terms = num_terms, RMSE = rmse_vals)
write.csv(rmse_v_terms_df, "LinearRegressionOutputs/rmse_v_terms.csv", row.names = F)
ggplot(rmse_v_terms_df) + geom_point(aes(num_terms, RMSE)) + labs(x = "Number of Terms")
```









