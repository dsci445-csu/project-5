---
title: "linear_regression_Hope"
output: pdf_document
---
```{r, message = F}
packages <- c("ggplot2", "tidyverse", "tidymodels", "olsrr")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
remove(packages, pkg)

set.seed(445)
```

```{r}
# read in data
all_data <- read.csv("~/DSCI445/project-5/CSV Files/merged_data.csv")
str(all_data)
```

```{r prep data}
# drop id and name columns in dataset for prediction
useful_columns <- all_data |> subset(select = -c(id, last_name, name))
useful_columns$weight <- as.numeric(useful_columns$weight)
useful_columns$height <- as.numeric(useful_columns$height)
useful_columns$attempted <- as.numeric(useful_columns$attempted)
useful_columns$aggravated <- as.numeric(useful_columns$aggravated)
useful_columns$armed <- as.numeric(useful_columns$armed)
useful_columns$life_sentence <- as.numeric(useful_columns$life_sentence)
useful_columns$death_sentence <- as.numeric(useful_columns$death_sentence)

useful_columns |> drop_na() -> useful_columns
# convert categorical variables to dummy variables and normalize numerical variables
prep_data <- recipe(current_sentence ~ ., data = useful_columns) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())
```

```{r set up tune}
# set up lasso tuning
cv_10fold <- vfold_cv(useful_columns, v = 10)

lasso_spec <- linear_reg(mixture = 1, penalty = tune("lambda")) |>
  set_mode("regression") |>
  set_engine("glmnet")

lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)
```

```{r tune}
set.seed(445)
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = cv_10fold, grid = tune_df) -> lasso_tune
```

```{r}
lasso_tune |>
  collect_metrics() |>
  select(lambda, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  ggplot() +
  geom_line(aes(lambda, rmse^2)) +
  geom_point(aes(lambda, rmse^2)) +
  coord_trans(x = "log10")

## best penalty
show_best(lasso_tune, metric = "rsq", n = 1)

save_rs <- rep(0, 4)
save_rs[1] <- show_best(lasso_tune, metric = "rsq", n = 1)$mean
```


```{r}
# tune chose smallest provided value for lambda; verify this is the best by providing smaller penalties
tune_df$lambda <- 10^seq(-5, -2, length.out = 100)

set.seed(445)
workflow() |>
  add_model(lasso_spec) |>
  add_recipe(prep_data) |>
  tune_grid(resamples = cv_10fold, grid = tune_df) -> lasso_tune

## best penalty
show_best(lasso_tune, metric = "rsq", n = 1)
save_rs[2] <- show_best(lasso_tune, metric = "rsq", n = 1)$mean
```

```{r}
set.seed(445)
model <- lm(current_sentence ~ ., data = useful_columns)
backward <- ols_step_backward_adj_r2(model)
backward$metrics$adj_r2
save_rs[3] <- max(backward$metrics$r2)
```

```{r}
set.seed(445)
forward <- ols_step_forward_adj_r2(model)
save_rs[4] <- max(forward$metrics$r2)
```

```{r}
which.max(save_rs)
backward
```
Backwards subset selection has the highest $R^2$ and so should be used for prediction.

```{r}
df_split <- initial_split(useful_columns, prop = 0.7)

df_train <- training(df_split)
df_test <- testing(df_split)

df_train <- df_train |> subset(select = -c(num_other_marks, sex, height))
```

```{r}
lm_spec <- linear_reg()
df_backwards <- df_train |> subset(select = -c(num_other_marks, sex, height))
backwards_recipe <- recipe(current_sentence ~ ., data = df_backwards)
backwards <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit(df_train)

get_rmse <- function(model) {
  augment(model, new_data = df_test) |>
    mutate(resid2 = .resid^2) |>
    summarise(rmse = sqrt(mean(resid2))) |>
    pull(rmse)
}

rmse_backwards <- get_rmse(backwards)
rmse_backwards
```

```{r}
# drops num_other_marks, height, sex
# reverse order num_scars, month_adm, hair, death_sentence, attempted, weight, eyes, armed, num_tattoos -- 10
# race, appx_age, aggravated, time_sentenced prior, offense_category
# top 5 in order: year_adm, life_sentence, class, parent_institution, total_counts
```

```{r}
ranked_variables <- forward$metrics$variable
num_terms <- 1:length(ranked_variables)
rmse_vals <- rep(0, length(num_terms))

for (i in num_terms){
  df_subset <- useful_columns |> subset(select = c(ranked_variables[1:i], "current_sentence"))
  subset_recipe <- recipe(current_sentence ~ ., data = df_subset)
  subset_fit <- workflow() |> add_model(lm_spec) |> add_recipe(subset_recipe) |> fit(df_train)
  
  rmse_vals[i] <- get_rmse(subset_fit)
}

rmse_v_terms_df <- data.frame(num_terms = num_terms, RMSE = rmse_vals)
ggplot(rmse_v_terms_df) + geom_point(aes(num_terms, RMSE))
```









